echo "multihead_attention_BCB_1"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/xglue_train.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 1 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_BCB_2"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/xglue_train.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 2 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_BCB_3"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/xglue_train.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 3 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_BCB_4"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/xglue_train.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 4 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_BCB_5"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/xglue_train.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 5 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_BCB_6"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/xglue_train.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 6 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_BCB_7"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/xglue_train.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 7 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_GCJ_1"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/GCJ/xglue_train.jsonl --eval_data_file=datasets/GCJ/eval_all.jsonl --test_data_file=datasets/GCJ/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 1 --d_size 128 --num_head 8 --pkl_file=datasets/GCJ/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_GCJ_2"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/GCJ/xglue_train.jsonl --eval_data_file=datasets/GCJ/eval_all.jsonl --test_data_file=datasets/GCJ/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 2 --d_size 128 --num_head 8 --pkl_file=datasets/GCJ/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_GCJ_3"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/GCJ/xglue_train.jsonl --eval_data_file=datasets/GCJ/eval_all.jsonl --test_data_file=datasets/GCJ/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 3 --d_size 128 --num_head 8 --pkl_file=datasets/GCJ/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_GCJ_4"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/GCJ/xglue_train.jsonl --eval_data_file=datasets/GCJ/eval_all.jsonl --test_data_file=datasets/GCJ/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 4 --d_size 128 --num_head 8 --pkl_file=datasets/GCJ/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_GCJ_5"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/GCJ/xglue_train.jsonl --eval_data_file=datasets/GCJ/eval_all.jsonl --test_data_file=datasets/GCJ/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 5 --d_size 128 --num_head 8 --pkl_file=datasets/GCJ/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_GCJ_6"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/GCJ/xglue_train.jsonl --eval_data_file=datasets/GCJ/eval_all.jsonl --test_data_file=datasets/GCJ/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 6 --d_size 128 --num_head 8 --pkl_file=datasets/GCJ/preprocess/path_embeddings_graph_10_v2.pkl
echo "multihead_attention_GCJ_7"
python run_multihead_attention.py --output_dir=./saved_models_1 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/GCJ/xglue_train.jsonl --eval_data_file=datasets/GCJ/eval_all.jsonl --test_data_file=datasets/GCJ/test_all.jsonl --epoch 2 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 7 --d_size 128 --num_head 8 --pkl_file=datasets/GCJ/preprocess/path_embeddings_graph_10_v2.pkl