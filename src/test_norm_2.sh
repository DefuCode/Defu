echo "BCB attention 8e-3"
python run_multihead_attention.py --output_dir=./saved_models_2 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/train_all.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 12 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 3 --lambda_reg 8e-3 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "BCB attention 7e-3"
python run_multihead_attention.py --output_dir=./saved_models_2 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/train_all.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 12 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 3 --lambda_reg 7e-3 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "BCB attention 5e-3"
python run_multihead_attention.py --output_dir=./saved_models_2 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/train_all.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 12 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 3 --lambda_reg 5e-3 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "BCB attention 3e-3"
python run_multihead_attention.py --output_dir=./saved_models_2 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/train_all.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 12 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 3 --lambda_reg 3e-3 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
echo "BCB attention 1e-3"
python run_multihead_attention.py --output_dir=./saved_models_2 --model_type=roberta --tokenizer_name=../models/graphcodebert --model_name_or_path=../models/graphcodebert --do_train --do_eval --train_data_file=datasets/BCB/train_all.jsonl --eval_data_file=datasets/BCB/eval_all.jsonl --test_data_file=datasets/BCB/test_all.jsonl --epoch 12 --block_size 512 --train_batch_size 36 --eval_batch_size 36 --learning_rate 1e-4 --max_grad_norm 1.0 --evaluate_during_training --seed 123456 --cnn_size 128 --filter_size 3 --lambda_reg 1e-3 --d_size 128 --num_head 8 --pkl_file=datasets/BCB/preprocess/path_embeddings_graph_10_v2.pkl
